{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96dd818-0b07-4295-b026-7497df750052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9200282a-67fe-4961-b2a5-f683d4f2be26",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1673359562.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 15\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f\"Error: Column \"{column_name}\" not found in the dataframe.\")\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"credit_risk_dataset – clean.csv\")\n",
    "print(df.head())\n",
    "\n",
    "# Helper function:\n",
    "# === For Quick Visual Check (numbers) ==========================================================\n",
    "def audit(column_name):\n",
    "    \"\"\"\n",
    "    Performs a basic statistical audit of a numerical column.\n",
    "    Displays descriptive statistics (describe), extreme values (TOP/BOTTOM 5), \n",
    "    and calculates the count and percentage of missing values (NaN).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for error    \n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in the dataframe.\")\n",
    "        return\n",
    "\n",
    "    print(f\"---  AUDIT: {column_name.upper()} ---\")\n",
    "    print(df[column_name].describe())\n",
    "\n",
    "    print(f\"\\nBOTTOM 5 ({column_name}):\")\n",
    "    print(df[column_name].nsmallest(5))\n",
    "\n",
    "    print(f\"\\nTOP 5 ({column_name}):\")\n",
    "    print(df[column_name].nlargest(5))\n",
    "\n",
    "    null_count = df[column_name].isnull().sum()\n",
    "    null_pct = (null_count / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\nMissing values: {null_count} ({null_pct:.2f} %)\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# === For capping outliers + boxplot ===========================================================\n",
    "def cap_outliers_iqr(column_name):\n",
    "    \"\"\"\n",
    "    Creates a boxplot and cap outliers from the specified attribute.\n",
    "    The cleaned data is stored in a new column named {column_name}_cleaned.\n",
    "    Displays a summary of the changes performed.\n",
    "    \"\"\"\n",
    "    # Check for error\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found.\")\n",
    "        return df\n",
    "\n",
    "    # Create boxplot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.boxplot(x=df[column_name], color=\"skyblue\")\n",
    "    plt.title(f\"Boxplot: {column_name}\")\n",
    "    plt.xlabel(\"Values\")\n",
    "    plt.show()\n",
    "\n",
    "    # IQR quartile calculation \n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Create new column\n",
    "    new_col_name = f\"{column_name}_cleaned\"\n",
    "    \n",
    "    # Check for same existing column\n",
    "    if new_col_name in df.columns:\n",
    "        df.drop(columns=[new_col_name], inplace=True)\n",
    "\n",
    "    # Capping\n",
    "    capped_series = df[column_name].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "    # Column insertion\n",
    "    idx = df.columns.get_loc(column_name)\n",
    "    df.insert(loc=idx + 1, column=new_col_name, value=capped_series)\n",
    "\n",
    "    # Audit calculations\n",
    "    num_capped_lower = (df[column_name] < lower_bound).sum()\n",
    "    num_capped_upper = (df[column_name] > upper_bound).sum()\n",
    "    total_capped = num_capped_lower + num_capped_upper\n",
    "    \n",
    "    print(f\"\\nAUDIT: {column_name.upper()}\")\n",
    "    print(f\"Boxplot limits: Low = {lower_bound:.2f}, High = {upper_bound:.2f}\")\n",
    "    \n",
    "    print(f\"\\n--- Statistics (Original vs Cleaned) ---\")\n",
    "    stats_comparison = pd.concat([df[column_name].describe(), df[new_col_name].describe()], axis=1)\n",
    "    print(stats_comparison)\n",
    "\n",
    "    print(f\"\\nValues capped (outside whiskers): {total_capped}\")\n",
    "    print(f\" - Capped below lower bound: {num_capped_lower}\")\n",
    "    print(f\" - Capped above upper bound: {num_capped_upper}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ae84c-7f6d-4a86-b5c8-d3e996ed7dd7",
   "metadata": {},
   "source": [
    "## Duplicates Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0a84b-0d39-4d81-9d1a-54c910f82c6e",
   "metadata": {},
   "source": [
    "Checking for duplicates. This dataset does not contain any unique keys, which presents two options: assume that identical rows are duplicates, or retain them because we cannot determine if they are actual duplicates or simply identical values for two distinct records. In this case, I will apply the second option (keeping them), as I lack a clear indication of whether they are duplicates. Furthermore, the number of such occurrences is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909f8d7-8d50-48b3-8c38-2840218204d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated()]\n",
    "print(f\"Number of duplicates: {len(duplicates)}\")\n",
    "\n",
    "duplicate_pairs = df[df.duplicated(keep=False)].sort_values(by=[\"person_income\", \"person_age\"])\n",
    "display(duplicate_pairs.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12011b2-652a-42bc-bf9f-9c71f64dbd77",
   "metadata": {},
   "source": [
    "## person_age Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845c0c5-77d0-4d69-81b3-20f7e8acd814",
   "metadata": {},
   "source": [
    "Since the data contains exaggerated values, I am capping age at 100. The dataset predominantly features younger individuals. While I could address outliers, they won't interfere with this specific analysis primarly focusing on the 25–30 age group. Therefore, I am only removing clear data entry errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a6e14-21dc-4708-8fe5-7196d10f10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean person_age + add column person_age_cleaned\n",
    "idx_age = df.columns.get_loc(\"person_age\")\n",
    "cleaned_data = df[\"person_age\"].mask(df[\"person_age\"] > 100)\n",
    "\n",
    "# Error prevention \n",
    "if \"person_age_cleaned\" in df.columns:\n",
    "    df.drop(columns=[\"person_age_cleaned\"], inplace=True)\n",
    "\n",
    "df.insert(loc=idx_age + 1, column=\"person_age_cleaned\", value=cleaned_data)\n",
    "\n",
    "# Control test\n",
    "audit(\"person_age\")\n",
    "audit(\"person_age_cleaned\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40224f05-5546-4f97-84ee-79a940a1cbc0",
   "metadata": {},
   "source": [
    "## person_emp_length Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af82b3e-da03-4dfa-99ce-f83a5f8eed81",
   "metadata": {},
   "source": [
    "For `person_emp_length` validation, I'm implementing a simple check: `person_emp_length` > (`person_age` - 15). This assumes a legal minimum working age of 15. Any records exceeding this threshold are considered invalid and removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a39e10-86e0-4bfb-b31b-a21f73b18748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate legal working age + add column person_emp_length_cleaned\n",
    "idx = df.columns.get_loc(\"person_emp_length\")\n",
    "employment_condition = df[\"person_emp_length\"] > (df[\"person_age\"] - 15)\n",
    "new_emp_col = df[\"person_emp_length\"].mask(employment_condition)\n",
    "\n",
    "# Error prevention \n",
    "if \"person_emp_length_cleaned\" in df.columns:\n",
    "    df.drop(columns=[\"person_emp_length_cleaned\"], inplace=True)\n",
    "\n",
    "df.insert(loc=idx + 1, column=\"person_emp_length_cleaned\", value=new_emp_col)\n",
    "invalid_count = df[\"person_emp_length_cleaned\"].isna().sum() - df[\"person_emp_length\"].isna().sum()\n",
    "print(f\"Number of records nullified due to logic check: {invalid_count}\")\n",
    "\n",
    "# Control test\n",
    "audit(\"person_emp_length\")\n",
    "audit(\"person_emp_length_cleaned\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c00aebb-275d-4967-876b-f4380d8b9462",
   "metadata": {},
   "source": [
    "## loan_percent_income Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80325a32-5557-4e80-88f3-31f88a0bdb51",
   "metadata": {},
   "source": [
    "In this section, I am validating the `loan_percent_income` field. First, I check for existing errors by comparing the current values with a manual calculation of `loan_amnt` / `person_income` to assess the extent of the discrepancies. I noticed that some errors stem from insufficient decimal precision (rounding to 2 decimal places or 0.00). To improve accuracy, I am increasing the precision to 4 decimal places and recalculating the values accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb90bb6-866b-4ff2-bd24-3506eef1490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan_percent_income - old data validation\n",
    "expected_pct_2 = (df[\"loan_amnt\"] / df[\"person_income\"]).round(2)\n",
    "errors_mask_2 = (df[\"loan_percent_income\"].round(2) != expected_pct_2)\n",
    "print(f\"Number of calculation inconsistencies (at 2 decimal places): {errors_mask_2.sum()}\")\n",
    "\n",
    "# loan_percent_income - new re-calculation\n",
    "expected_pct_4 = (df[\"loan_amnt\"] / df[\"person_income\"]).round(4)\n",
    "new_col_values = df[\"loan_percent_income\"].copy()\n",
    "errors_mask_4 = (df[\"loan_percent_income\"].round(4) != expected_pct_4)\n",
    "new_col_values[errors_mask_4] = expected_pct_4[errors_mask_4]\n",
    "idx = df.columns.get_loc(\"loan_percent_income\")\n",
    "\n",
    "# Error prevention \n",
    "if \"loan_percent_income_cleaned\" in df.columns:\n",
    "    df.drop(columns=[\"loan_percent_income_cleaned\"], inplace=True)\n",
    "\n",
    "df.insert(loc=idx + 1, column=\"loan_percent_income_cleaned\", value=new_col_values)\n",
    "\n",
    "print(f\"Successfully created \"loan_percent_income_cleaned\" with 4-decimal precision.\")\n",
    "\n",
    "audit(\"loan_percent_income\")\n",
    "audit(\"loan_percent_income_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59808a72-2992-452b-82ab-cbcc2c17bf3f",
   "metadata": {},
   "source": [
    "## cb_person_cred_hist_length Flagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4bc76-091b-470c-bb09-d86b9d4db67f",
   "metadata": {},
   "source": [
    "In this step, I am flagging cases where the credit history starts before the age of 15. I've set this as an auxiliary threshold to assess the quality of the dataset and understand the distribution of these potentially problematic records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f812b1c-1b31-464e-b88e-b24b40ad4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags suspicious records\n",
    "invalid_logic = df[(df[\"person_age\"] - df[\"cb_person_cred_hist_length\"]) < 15]\n",
    "print(f\"Number of inconsistent records identified: {len(invalid_logic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee31f28-0c21-4897-8b7a-8375400f3dfa",
   "metadata": {},
   "source": [
    "## person_income Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c018a8-3f11-4d86-8179-715ae4ba9f90",
   "metadata": {},
   "source": [
    "In this step, I am capping outliers in person_income using a 'Winsorization' approach based on the Interquartile Range (IQR). As shown in the initial boxplot, the presence of an extreme outlier (6,000,000) distorts the visualization to the point where the boxplot becomes unreadable. To ensure a more robust analysis and better data distribution, it is necessary to cap these extreme values at the whiskers of the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613da907-71e3-4d6a-a93e-03e393ac5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_outliers_iqr(\"person_income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682205e5-7b40-4b6a-bf4f-e6984cabe137",
   "metadata": {},
   "source": [
    "## loan_amnt Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c4207-b6bf-4c06-b5fd-9e440694a289",
   "metadata": {},
   "source": [
    "Applying the same IQR-based Winsorization logic to `loan_amnt` to handle outliers and maintain consistency across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239fabe-fa18-4699-968c-9bc51d2bbcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_outliers_iqr(\"loan_amnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a52b0-81a2-4908-9dab-26c1862bda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data\n",
    "df.to_csv(\"data_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
